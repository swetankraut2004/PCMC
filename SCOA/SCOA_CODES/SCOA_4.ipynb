{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKLG2DbKrzSB",
    "outputId": "bb8f63b5-85f7-4900-da83-c9318734c468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0 - Best Fitness: 0.9667\n",
      "Generation 1 - Best Fitness: 0.9667\n",
      "Generation 2 - Best Fitness: 0.9667\n",
      "Generation 3 - Best Fitness: 0.9667\n",
      "Generation 4 - Best Fitness: 0.9667\n",
      "Generation 5 - Best Fitness: 0.9667\n",
      "Generation 6 - Best Fitness: 0.9667\n",
      "Generation 7 - Best Fitness: 0.9667\n",
      "Generation 8 - Best Fitness: 0.9667\n",
      "Generation 9 - Best Fitness: 0.9667\n",
      "Best Hyperparameters: [8, 10]\n",
      "Best Accuracy: 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# --- Genetic Algorithm Setup ---\n",
    "POP_SIZE = 20      # number of individuals\n",
    "N_GENERATIONS = 10 # iterations\n",
    "MUTATION_RATE = 0.2\n",
    "\n",
    "# Chromosome: [max_depth, min_samples_split]\n",
    "def create_chromosome():\n",
    "    return [random.randint(1, 20), random.randint(2, 10)]\n",
    "\n",
    "def fitness(chromosome):\n",
    "    max_depth, min_samples_split = chromosome\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth,\n",
    "                                   min_samples_split=min_samples_split)\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    return scores.mean()\n",
    "\n",
    "def selection(population, fitnesses):\n",
    "    idx = np.argsort(fitnesses)[-2:]  # select best two\n",
    "    return [population[idx[0]], population[idx[1]]]\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    point = random.randint(0, len(parent1)-1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "def mutate(chromosome):\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        chromosome[0] = random.randint(1, 20)\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        chromosome[1] = random.randint(2, 10)\n",
    "    return chromosome\n",
    "\n",
    "# --- Run GA ---\n",
    "population = [create_chromosome() for _ in range(POP_SIZE)]\n",
    "\n",
    "for gen in range(N_GENERATIONS):\n",
    "    fitnesses = [fitness(chromo) for chromo in population]\n",
    "    print(f\"Generation {gen} - Best Fitness: {max(fitnesses):.4f}\")\n",
    "\n",
    "    new_population = []\n",
    "    parents = selection(population, fitnesses)\n",
    "    for _ in range(POP_SIZE // 2):\n",
    "        child1, child2 = crossover(parents[0], parents[1])\n",
    "        new_population.append(mutate(child1))\n",
    "        new_population.append(mutate(child2))\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Best result\n",
    "fitnesses = [fitness(chromo) for chromo in population]\n",
    "best_idx = np.argmax(fitnesses)\n",
    "print(\"Best Hyperparameters:\", population[best_idx])\n",
    "print(\"Best Accuracy:\", fitnesses[best_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Zoom ‚Äî Assignment No. 4\n",
    "\n",
    "## Title: Optimization of Machine Learning Model Parameters using Genetic Algorithm\n",
    "\n",
    "### Objectives\n",
    "1. To understand the concept of Genetic Algorithms and their role in optimization.\n",
    "2. To apply GA for tuning hyperparameters of a machine learning model.\n",
    "3. To compare GA-based optimization with traditional approaches like grid search or random search.\n",
    "4. To evaluate the performance improvement of the ML model after optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### Theory (summary)\n",
    "A Genetic Algorithm (GA) is a population-based optimization method inspired by natural selection. It encodes candidate solutions as chromosomes and iteratively applies selection, crossover, and mutation to evolve better solutions according to a fitness function. GAs are well-suited for non-differentiable and multi-modal search spaces common in hyperparameter tuning.\n",
    "\n",
    "### Mapping theory to the notebook code\n",
    "The code implements a simple GA that tunes two hyperparameters for a Decision Tree: `max_depth` and `min_samples_split`. Below is a line-by-line mapping between the GA steps described in the assignment and the implementation.\n",
    "\n",
    "1) Representation (Chromosome):\n",
    "   - In the code a chromosome is a Python list `[max_depth, min_samples_split]`. Each gene is an integer sampled in the allowed range. This is a direct integer encoding; other encodings (binary, real-valued vectors) are possible depending on the parameter type.\n",
    "\n",
    "2) Population Initialization:\n",
    "   - `create_chromosome()` randomly samples each gene: `max_depth` ‚àà [1,20], `min_samples_split` ‚àà [2,10].\n",
    "   - `population = [create_chromosome() for _ in range(POP_SIZE)]` creates the initial population of size `POP_SIZE`.\n",
    "\n",
    "3) Fitness Evaluation:\n",
    "   - `fitness(chromosome)` builds a `DecisionTreeClassifier` with the chromosome parameters and returns the mean cross-validation accuracy via `cross_val_score(..., cv=5)`. The returned mean score is the fitness value (higher is better).\n",
    "   - This measures generalization performance rather than training accuracy, which helps avoid overfitting during optimization.\n",
    "\n",
    "4) Selection:\n",
    "   - The code uses a simple deterministic selection `selection(population, fitnesses)` that picks the two best individuals via `np.argsort(fitnesses)[-2:]`.\n",
    "   - This is elitist: always keeping the best two as parents. Alternative probabilistic methods (roulette wheel, tournament) would preserve more diversity.\n",
    "\n",
    "5) Crossover (Recombination):\n",
    "   - `crossover(parent1, parent2)` picks a single crossover point and swaps tails to create two children. With a 2-gene chromosome the single-point crossover either swaps the second gene or not (point = 0 or 1).\n",
    "   - This inherits genes from both parents in different combinations.\n",
    "\n",
    "6) Mutation:\n",
    "   - `mutate(chromosome)` with probability `MUTATION_RATE` replaces each gene by a random value within its allowed range. This injects new genetic material and prevents premature convergence.\n",
    "\n",
    "7) Replacement (Survivor Selection):\n",
    "   - The implementation uses generational replacement: after generating `POP_SIZE` children the old population is replaced with the new one (`population = new_population`).\n",
    "   - Note: no elitism is used here to retain the absolute best from the previous generation ‚Äî you can add elitism by carrying over the top individual(s) to the new population unchanged.\n",
    "\n",
    "8) Termination Criteria:\n",
    "   - The loop runs for `N_GENERATIONS`. You can also add early stopping by monitoring fitness improvement or reaching a target accuracy.\n",
    "\n",
    "### Practical and implementation notes\n",
    "- Chromosome validation: ensure generated chromosomes are within valid ranges ‚Äî current code samples valid ranges directly.\n",
    "- Fitness cost: evaluating fitness uses cross-validation and can be expensive. For real datasets consider fewer CV folds, holdout validation, or parallelization to speed up evaluations.\n",
    "- Selection pressure: deterministic selection of top-2 parents reduces diversity. Consider tournament or roulette selection for greater exploration.\n",
    "- Elitism: keep the best individual(s) from previous generation to avoid losing high-quality solutions.\n",
    "- Mutation strategy: replacing with a random value is simple; Gaussian perturbation (for continuous genes) or small step mutations can be more effective.\n",
    "- Crossover details: with multi-gene chromosomes use two-point or uniform crossover for richer recombination.\n",
    "\n",
    "### Complexity and performance\n",
    "- Time complexity per generation ‚âà O(POP_SIZE √ó cost_of_fitness). Fitness here includes training and cross-validation of a model; this dominates runtime.\n",
    "- For faster experiments: reduce `POP_SIZE`, `N_GENERATIONS`, or CV folds; or parallelize `fitness` evaluations with joblib or multiprocessing.\n",
    "\n",
    "### Comparison with grid and random search\n",
    "- Grid search exhaustively enumerates a predefined grid ‚Äî reliable but scales poorly with dimensionality (curse of dimensionality).\n",
    "- Random search samples the space randomly ‚Äî often more efficient than grid search for high-dimensional spaces because it explores more unique values per budget.\n",
    "- GA is a guided stochastic search: it leverages selection and recombination to focus on promising regions and can outperform random/grid search given a reasonable budget and properly tuned GA operators. However GAs can be more complex to tune (population size, mutation rate, selection strategy).\n",
    "\n",
    "### Example outputs and interpretation\n",
    "- The notebook prints the best fitness each generation and the final best hyperparameters and accuracy.\n",
    "- Verify that the best accuracy increases (or at least doesn't degrade) across generations. If results are noisy, run multiple GA runs and average results.\n",
    "\n",
    "### Extensions and improvements\n",
    "- Add elitism to preserve best individuals.\n",
    "- Use tournament selection or roulette wheel selection to maintain diversity.\n",
    "- Encode continuous hyperparameters as real-valued genes and use arithmetic crossover and Gaussian mutation.\n",
    "- Parallelize fitness evaluations with joblib or multiprocessing.\n",
    "- Use surrogate models (e.g., Gaussian processes, tree ensembles) to approximate fitness and reduce expensive evaluations.\n",
    "\n",
    "### Conclusion\n",
    "This markdown links the GA theory to the concrete implementation in the notebook: chromosome design via `create_chromosome`, population initialization, evaluation through `fitness`, selection, crossover, mutation, and generational replacement. The method shows how GA can be applied to hyperparameter optimization for ML models and highlights practical tradeoffs compared to grid and random search.\n",
    "\n",
    "If you want, I can also: (a) add comments inline in the code cells to explain each function, (b) refactor the GA to include elitism and tournament selection, or (c) add an experiment comparison between GA, grid search and random search on the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GA Code ‚Äî Line-by-line Explanation and Output (Assignment 4)\n",
    "\n",
    "This cell explains the GA implementation (the code in the previous cell) line-by-line, describes the printed outputs you will see when the notebook runs, and lists edge-cases and recommended improvements.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Imports and data\n",
    "- `import numpy as np` ‚Äî imports NumPy (alias `np`) used for sorting and numeric utilities.\n",
    "- `import random` ‚Äî Python's pseudo-random generator used for sampling integers and floats.\n",
    "- `from sklearn.datasets import load_iris` ‚Äî loads the Iris dataset used for experiments.\n",
    "- `from sklearn.model_selection import cross_val_score` ‚Äî evaluates model performance using cross-validation.\n",
    "- `from sklearn.tree import DecisionTreeClassifier` ‚Äî the ML model whose hyperparameters are tuned.\n",
    "\n",
    "## 2) Dataset loading\n",
    "- `iris = load_iris()` and `X, y = iris.data, iris.target` load the feature matrix and target vector. Iris has 150 samples and 3 classes, commonly used for small experiments.\n",
    "\n",
    "## 3) GA hyperparameters\n",
    "- `POP_SIZE = 20` sets the population size.\n",
    "- If you want, I can now apply one of the improvements to the notebook code (e.g., tournament selection + elitism + reproducible seed) and run a single deterministic example to show exact printed outputs.- This GA is a clear, educational skeleton for hyperparameter optimization. For production or rigorous experiments, add reproducibility, parallelism, better selection/crossover/mutation, and logging.## Final notes---- To make results reproducible: add at the top `random.seed(0); np.random.seed(0)`.  - Implement `def tournament(pop, fits, k=3):` that samples `k` indices without replacement and returns the individual with highest fitness; call it twice for two parents per mating.- To use tournament selection for each mating pair (k=3):  - Evaluate fitnesses, find best index `best_idx`, then `new_population.append(population[best_idx])` before generating children, and generate `POP_SIZE-1` children afterwards.- To add simple elitism (keep top 1):## Quick code changes you can apply (snippets)---   - Fix: ensure children are copies (e.g., `child1.copy()`) before mutation or construct new lists explicitly.   - Problem: mutation modifies lists in place; if parents and children share list objects this may cause side effects.7. In-place mutation and shared references:   - Fix: explicitly handle odd POP_SIZE by producing one extra child or appending a copied individual.   - Problem: `POP_SIZE // 2` assumes an even population size; odd sizes will produce fewer children.6. Odd `POP_SIZE` handling:   - Fix: parallelize fitness evaluations (joblib.Parallel or multiprocessing) or reduce CV folds for faster experiments. Consider caching evaluations for identical hyperparameters.   - Problem: cross-validation inside fitness is expensive.5. Fitness evaluation cost/parallelism:   - Fix: implement tournament selection (choose k random individuals, pick the best) or roulette wheel selection to allow some weaker individuals to reproduce.   - Problem: deterministic selection of top-2 removes stochasticity.4. Selection method reduces exploration:   - Fix: use small-step mutation (¬±1), or for continuous genes use Gaussian perturbation. Alternatively, adapt mutation rate over time.   - Problem: replacing an integer gene with a completely random value is extreme and may destroy good traits.3. Mutation strategy is disruptive:   - Fix: carry over the top-k individuals unchanged into the new population (elitism).   - Problem: replacing the whole population may discard the best solutions found so far.2. No elitism (best individuals can be lost):   - Fix: select parent pairs anew for each mating (e.g., tournament selection) or sample parents with probability proportional to fitness.   - Problem: the code selects only the top-2 parents and mates them repeatedly to generate the whole next population. This reduces genetic diversity drastically and can cause premature convergence.1. Low diversity due to single-parent-pair mating:## Common issues, edge cases, and suggestions (practical improvements)---Notes on the output: the numeric values will vary between runs because chromosome initialization, crossover point, and mutation are random. To reproduce a run, set seeds `random.seed(...)` and `np.random.seed(...)` before initializing the population.  - Best Accuracy: 0.9533333333333334  - Best Hyperparameters: [5, 2]- At the end you'll see something like:  - ...  - Generation 1 - Best Fitness: 0.9467  - Generation 0 - Best Fitness: 0.9333- During execution you will see one line per generation, e.g.:## What you will see when you run the cell (example output)---- After generations finish, the final population's fitnesses are evaluated again, `np.argmax` finds the index of the best chromosome, and the script prints:\n",
    "  - `Best Hyperparameters: [max_depth, min_samples_split]`\n",
    "  - `Best Accuracy: <mean_cv_accuracy>`\n",
    "- These are the best solution found in the final generation (not strictly the global best seen during the entire run unless elitism was used).## 10) Final selection and output- `population = [create_chromosome() for _ in range(POP_SIZE)]` initializes the population.\n",
    "- For each generation `gen` in `range(N_GENERATIONS)` the code: evaluates fitness for all chromosomes; prints `Generation {gen} - Best Fitness: {value}` where `value` is the highest mean CV accuracy among the population; selects the two best parents; repeatedly performs crossover and mutation `POP_SIZE//2` times to form `new_population`; replaces the old population with `new_population` (generational replacement).## 9) Main GA loop and generation replacement- `mutate(chromosome)` applies per-gene replacement with probability `MUTATION_RATE`. When triggered, the gene is replaced with a new random value from its domain (full replacement mutation). This is simple but can be disruptive compared to small-step mutations.## 8) Mutation- `crossover(parent1, parent2)` chooses a random crossover `point` in `[0, len(parent1)-1]` and builds two children by slicing and concatenation: `child1 = parent1[:point] + parent2[point:]` and vice versa.\n",
    "- With two genes, `point` is 0 or 1: `point==0` swaps entire chromosomes, `point==1` swaps only the second gene.## 7) Crossover (recombination)- `selection(population, fitnesses)` uses `np.argsort(fitnesses)[-2:]` to pick indices of the top two fitnesses and returns those chromosomes as parents.\n",
    "- This selection is deterministic and always returns the two best individuals in the current population.## 6) Selection- `fitness(chromosome)` unpacks the two genes and instantiates `DecisionTreeClassifier` with those hyperparameters. It runs `cross_val_score(..., cv=5)` and returns the mean accuracy across 5 folds as the fitness value.\n",
    "- Note: the fitness call trains and evaluates the model 5 times ‚Äî this is the most expensive operation in the GA loop.## 5) Fitness function- `create_chromosome()` returns `[max_depth, min_samples_split]` where `max_depth` ‚àà [1,20] and `min_samples_split` ‚àà [2,10]. Each chromosome is a Python list (two integer genes).## 4) Chromosome representation and creation- `N_GENERATIONS = 10` sets the number of generations to run.\n",
    "- `MUTATION_RATE = 0.2` is the per-gene mutation probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GA Code ‚Äî Detailed Description (neutral)\n",
    "\n",
    "The following describes, in detail and without recommendations, what each part of the GA code cell does and what outputs are produced when it runs.\n",
    "\n",
    "---\n",
    "\n",
    "### Imports and dataset\n",
    "- `import numpy as np`: imports NumPy for numeric operations and utilities (used for argsort and array operations).\n",
    "- `import random`: imports Python's random module for sampling random integers and floats.\n",
    "- `from sklearn.datasets import load_iris`: imports the Iris dataset loader.\n",
    "- `from sklearn.model_selection import cross_val_score`: imports cross-validation utility that returns a score array.\n",
    "- `from sklearn.tree import DecisionTreeClassifier`: imports the Decision Tree classifier used in fitness evaluation.\n",
    "- `iris = load_iris()` and `X, y = iris.data, iris.target`: load features `X` and labels `y` from the Iris dataset.\n",
    "\n",
    "### GA hyperparameters and configuration\n",
    "- `POP_SIZE = 20`: sets the number of individuals in the population.\n",
    "- `N_GENERATIONS = 10`: sets how many generations the GA will run.\n",
    "- `MUTATION_RATE = 0.2`: per-gene probability of mutation during the mutate step.\n",
    "\n",
    "### Chromosome representation and creation\n",
    "- Chromosome structure: a Python list of two integers `[max_depth, min_samples_split]`.\n",
    "- `create_chromosome()` returns a new chromosome by sampling `max_depth` uniformly from 1..20 and `min_samples_split` from 2..10 using `random.randint`.\n",
    "\n",
    "### Fitness function\n",
    "- `fitness(chromosome)` unpacks the genes into `max_depth` and `min_samples_split`.\n",
    "- It creates a `DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split)`.\n",
    "- It computes cross-validated scores with `cross_val_score(model, X, y, cv=5)` and returns the mean of these scores (`scores.mean()`).\n",
    "- The returned mean accuracy is used by the GA as the fitness value to compare chromosomes.\n",
    "\n",
    "### Selection\n",
    "- `selection(population, fitnesses)` computes `np.argsort(fitnesses)[-2:]` to obtain indices of the two highest fitness values.\n",
    "- It returns the two corresponding chromosomes from `population` as the parent pair for crossover.\n",
    "\n",
    "### Crossover\n",
    "- `crossover(parent1, parent2)` selects a crossover `point = random.randint(0, len(parent1)-1)`.\n",
    "- It forms `child1 = parent1[:point] + parent2[point:]` and `child2 = parent2[:point] + parent1[point:]`.\n",
    "- For a two-gene chromosome `point` is either 0 or 1; `point==0` yields children equal to the parents swapped, `point==1` swaps only the second gene between parents.\n",
    "\n",
    "### Mutation\n",
    "- `mutate(chromosome)` checks `random.random() < MUTATION_RATE` for each gene. If true, it replaces the gene with a new random integer from the gene's domain (`random.randint(1,20)` for the first gene, `random.randint(2,10)` for the second).\n",
    "- The function returns the (possibly modified) chromosome.\n",
    "\n",
    "### Main GA loop\n",
    "- `population = [create_chromosome() for _ in range(POP_SIZE)]` initializes the population with POP_SIZE random chromosomes.\n",
    "- For each generation `gen` in `range(N_GENERATIONS)`:\n",
    "  - `fitnesses = [fitness(chromo) for chromo in population]` evaluates fitness for all individuals in the population and stores the mean CV accuracy values in a list.\n",
    "  - `print(f\"Generation {gen} - Best Fitness: {max(fitnesses):.4f}\")` prints the highest fitness value from the current population formatted to four decimals.\n",
    "  - `parents = selection(population, fitnesses)` obtains the top two individuals.\n",
    "  - The code enters a loop `for _ in range(POP_SIZE // 2):` and in each iteration: calls `crossover(parents[0], parents[1])` to create two children, applies `mutate` to each child, and appends them to `new_population`.\n",
    "  - After the loop `population = new_population` replaces the old generation with the newly produced children.\n",
    "\n",
    "### Final evaluation and printed output\n",
    "- After the GA loop completes: `fitnesses = [fitness(chromo) for chromo in population]` computes fitnesses for the final population.\n",
    "- `best_idx = np.argmax(fitnesses)` finds the index of the chromosome with maximum fitness in the final population.\n",
    "- `print(\"Best Hyperparameters:\", population[best_idx])` prints the chromosome (list of two integers) that had the best fitness in the final population.\n",
    "- `print(\"Best Accuracy:\", fitnesses[best_idx])` prints the corresponding mean CV accuracy value.\n",
    "\n",
    "### Example of runtime printed lines\n",
    "- During the run the notebook prints one `Generation` line per generation indicating the best fitness in that generation, for example: `Generation 0 - Best Fitness: 0.9333`.\n",
    "- At the end it prints two lines: `Best Hyperparameters: [max_depth, min_samples_split]` and `Best Accuracy: <value>`.\n",
    "\n",
    "### Notes on random behavior\n",
    "- The code uses the `random` module (and NumPy in selection), so successive runs may produce different populations, crossover points, mutations, and therefore different printed fitness progress and final hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "End of detailed description: the content above enumerates each function and code block and the exact outputs the script generates during execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect ‚Äî this code implements a **Genetic Algorithm (GA)** to **optimize hyperparameters** of a Decision Tree classifier on the Iris dataset. Let‚Äôs go line by line and conceptually unpack everything (including design choices and possible improvements).\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Imports\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "```\n",
    "\n",
    "* `numpy` ‚Äî for numerical operations (arrays, sorting indices, etc.).\n",
    "* `random` ‚Äî for random integer generation and probabilities (used for GA operations).\n",
    "* `load_iris` ‚Äî loads the classic *Iris dataset* (a small 3-class classification dataset).\n",
    "* `cross_val_score` ‚Äî performs k-fold cross-validation to measure model accuracy.\n",
    "* `DecisionTreeClassifier` ‚Äî model whose hyperparameters we will optimize using the GA.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Load dataset\n",
    "\n",
    "```python\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "```\n",
    "\n",
    "* Loads 150 samples of Iris flowers (4 numeric features per sample).\n",
    "* `X`: features (shape = [150, 4])\n",
    "* `y`: labels (0, 1, or 2)\n",
    "\n",
    "This will be used to train and validate decision trees.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Genetic Algorithm configuration\n",
    "\n",
    "```python\n",
    "POP_SIZE = 20      # number of individuals in the population\n",
    "N_GENERATIONS = 10 # number of iterations (evolution steps)\n",
    "MUTATION_RATE = 0.2 # probability of mutation\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Each *individual* in the population encodes a set of hyperparameters.\n",
    "* The GA will evolve these individuals over 10 generations.\n",
    "* At each generation, it applies *selection ‚Üí crossover ‚Üí mutation*.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Chromosome representation\n",
    "\n",
    "```python\n",
    "# Chromosome: [max_depth, min_samples_split]\n",
    "def create_chromosome():\n",
    "    return [random.randint(1, 20), random.randint(2, 10)]\n",
    "```\n",
    "\n",
    "Each **chromosome** is a list of two integers:\n",
    "\n",
    "* `max_depth`: controls how deep the decision tree can grow.\n",
    "* `min_samples_split`: minimum number of samples required to split a node.\n",
    "\n",
    "So, an example chromosome might be `[10, 3]`.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Fitness function\n",
    "\n",
    "```python\n",
    "def fitness(chromosome):\n",
    "    max_depth, min_samples_split = chromosome\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth,\n",
    "                                   min_samples_split=min_samples_split)\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    return scores.mean()\n",
    "```\n",
    "\n",
    "**Purpose:** evaluate how *good* each chromosome (hyperparameter set) is.\n",
    "\n",
    "**Step-by-step:**\n",
    "\n",
    "1. Extract the two hyperparameters from the chromosome.\n",
    "2. Build a `DecisionTreeClassifier` with those parameters.\n",
    "3. Evaluate it using 5-fold cross-validation.\n",
    "4. Return the *mean accuracy* as the fitness value.\n",
    "\n",
    "**Interpretation:**\n",
    "Higher fitness ‚Üí better performing hyperparameters.\n",
    "\n",
    "**Cost:**\n",
    "Each evaluation trains and tests 5 models, so this step is the computational bottleneck.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Selection (choose best parents)\n",
    "\n",
    "```python\n",
    "def selection(population, fitnesses):\n",
    "    idx = np.argsort(fitnesses)[-2:]  # select best two\n",
    "    return [population[idx[0]], population[idx[1]]]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Sort fitness values and select indices of top 2 individuals (highest fitness).\n",
    "* Return those two as the ‚Äúparents‚Äù for the next generation.\n",
    "\n",
    "**Note:** This is **elitist selection** ‚Äî only best individuals are kept as breeding parents.\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Crossover (recombination)\n",
    "\n",
    "```python\n",
    "def crossover(parent1, parent2):\n",
    "    point = random.randint(0, len(parent1)-1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Randomly choose a crossover point between genes.\n",
    "* For our 2-gene chromosomes:\n",
    "\n",
    "  * If point = 0 ‚Üí children swap almost everything.\n",
    "  * If point = 1 ‚Üí first element from parent1, second from parent2 (and vice versa).\n",
    "* Returns two children.\n",
    "\n",
    "**Purpose:** allows mixing of hyperparameter combinations.\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Mutation (random variation)\n",
    "\n",
    "```python\n",
    "def mutate(chromosome):\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        chromosome[0] = random.randint(1, 20)\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        chromosome[1] = random.randint(2, 10)\n",
    "    return chromosome\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* With probability `MUTATION_RATE`, randomly reassign one or both genes.\n",
    "* Helps avoid local optima and ensures genetic diversity.\n",
    "\n",
    "**Effect:** occasional random jumps in parameter space.\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Initialize population\n",
    "\n",
    "```python\n",
    "population = [create_chromosome() for _ in range(POP_SIZE)]\n",
    "```\n",
    "\n",
    "Generates 20 random individuals ‚Äî the initial population.\n",
    "\n",
    "Example:\n",
    "`[[6, 3], [12, 5], [8, 4], ‚Ä¶]`\n",
    "\n",
    "---\n",
    "\n",
    "## üîü Main Genetic Algorithm loop\n",
    "\n",
    "```python\n",
    "for gen in range(N_GENERATIONS):\n",
    "    fitnesses = [fitness(chromo) for chromo in population]\n",
    "    print(f\"Generation {gen} - Best Fitness: {max(fitnesses):.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Evaluate fitness for all individuals in the population.\n",
    "* Print the best accuracy (fitness) for monitoring progress.\n",
    "\n",
    "---\n",
    "\n",
    "### Create next generation\n",
    "\n",
    "```python\n",
    "    new_population = []\n",
    "    parents = selection(population, fitnesses)\n",
    "```\n",
    "\n",
    "Select the top 2 individuals (parents).\n",
    "\n",
    "---\n",
    "\n",
    "### Reproduce (crossover + mutation)\n",
    "\n",
    "```python\n",
    "    for _ in range(POP_SIZE // 2):\n",
    "        child1, child2 = crossover(parents[0], parents[1])\n",
    "        new_population.append(mutate(child1))\n",
    "        new_population.append(mutate(child2))\n",
    "```\n",
    "\n",
    "**Process:**\n",
    "\n",
    "* Repeat to create enough children to refill population.\n",
    "* Each iteration produces 2 offspring via crossover and mutation.\n",
    "* After the loop, `new_population` will again have 20 individuals.\n",
    "\n",
    "---\n",
    "\n",
    "### Replace old population\n",
    "\n",
    "```python\n",
    "    population = new_population\n",
    "```\n",
    "\n",
    "The next generation completely replaces the old one (no elitism retention in this code ‚Äî could be added for better convergence).\n",
    "\n",
    "---\n",
    "\n",
    "## 11Ô∏è‚É£ Final evaluation\n",
    "\n",
    "```python\n",
    "fitnesses = [fitness(chromo) for chromo in population]\n",
    "best_idx = np.argmax(fitnesses)\n",
    "print(\"Best Hyperparameters:\", population[best_idx])\n",
    "print(\"Best Accuracy:\", fitnesses[best_idx])\n",
    "```\n",
    "\n",
    "At the end:\n",
    "\n",
    "1. Re-evaluate final generation.\n",
    "2. Identify the chromosome with the highest fitness.\n",
    "3. Print its hyperparameters and achieved accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 12Ô∏è‚É£ Conceptual Summary\n",
    "\n",
    "| Stage                         | GA Component          | Function                                   | Purpose |\n",
    "| ----------------------------- | --------------------- | ------------------------------------------ | ------- |\n",
    "| **Population initialization** | `create_chromosome()` | Random starting solutions                  |         |\n",
    "| **Fitness evaluation**        | `fitness()`           | Measures accuracy of each individual       |         |\n",
    "| **Selection**                 | `selection()`         | Picks the fittest parents                  |         |\n",
    "| **Crossover**                 | `crossover()`         | Mixes parent genes to create new offspring |         |\n",
    "| **Mutation**                  | `mutate()`            | Introduces random variations               |         |\n",
    "| **Replacement**               | Loop update           | Forms next generation                      |         |\n",
    "\n",
    "---\n",
    "\n",
    "## 13Ô∏è‚É£ Example Flow (intuitive)\n",
    "\n",
    "1. Start with 20 random `[max_depth, min_samples_split]` pairs.\n",
    "2. Evaluate all ‚Üí find the 2 best.\n",
    "3. Mix them (crossover) and randomly mutate ‚Üí form 20 new children.\n",
    "4. Repeat 10 times ‚Üí parameters should converge toward the best-performing combination.\n",
    "\n",
    "---\n",
    "\n",
    "## 14Ô∏è‚É£ Limitations & Improvements\n",
    "\n",
    "| Issue                                            | Suggestion                                                       |\n",
    "| ------------------------------------------------ | ---------------------------------------------------------------- |\n",
    "| Only top-2 parents used ‚Üí low diversity          | Use roulette-wheel or tournament selection                       |\n",
    "| Entire population replaced ‚Üí risk of losing best | Keep best (elitism) in next gen                                  |\n",
    "| Small population and few generations             | Increase for better exploration                                  |\n",
    "| Only 2 parameters tuned                          | Could expand to more hyperparameters (criterion, splitter, etc.) |\n",
    "| Slow evaluation                                  | Use parallel computing or fewer CV folds                         |\n",
    "\n",
    "---\n",
    "\n",
    "## 15Ô∏è‚É£ Intuitive Takeaway\n",
    "\n",
    "The **GA acts like a biologically inspired search algorithm** that ‚Äúevolves‚Äù good hyperparameters by:\n",
    "\n",
    "* Selecting better performers (survival of the fittest),\n",
    "* Mixing their traits (crossover),\n",
    "* Occasionally introducing random changes (mutation).\n",
    "\n",
    "Instead of grid or random search, this uses guided evolution based on performance.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to **rewrite this GA to be more robust and faster** (e.g., with elitism, adaptive mutation rate, and parallelized fitness evaluation)? It would show a more research-ready version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
