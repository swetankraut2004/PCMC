{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7knOacyey7to",
        "outputId": "72d23918-b1b9-45ec-e40a-acd573294910"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_14216\\3594839966.py:12: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start='2020-01-01', end='2023-01-01')\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "C:\\Users\\sweta\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4967 - loss: 0.6940   \n",
            "Epoch 2/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5232 - loss: 0.6924 \n",
            "Epoch 3/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5215 - loss: 0.6915 \n",
            "Epoch 4/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5248 - loss: 0.6913 \n",
            "Epoch 5/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5248 - loss: 0.6922 \n",
            "Epoch 6/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5215 - loss: 0.6914 \n",
            "Epoch 7/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5083 - loss: 0.6909 \n",
            "Epoch 8/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5166 - loss: 0.6910 \n",
            "Epoch 9/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5298 - loss: 0.6907 \n",
            "Epoch 10/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5232 - loss: 0.6911 \n",
            "Epoch 11/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5132 - loss: 0.6907 \n",
            "Epoch 12/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5149 - loss: 0.6913 \n",
            "Epoch 13/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5199 - loss: 0.6908 \n",
            "Epoch 14/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5215 - loss: 0.6903 \n",
            "Epoch 15/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5248 - loss: 0.6899 \n",
            "Epoch 16/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5232 - loss: 0.6904 \n",
            "Epoch 17/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5248 - loss: 0.6902 \n",
            "Epoch 18/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5215 - loss: 0.6898 \n",
            "Epoch 19/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5414 - loss: 0.6897 \n",
            "Epoch 20/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5199 - loss: 0.6895 \n",
            "Epoch 21/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5381 - loss: 0.6895 \n",
            "Epoch 22/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5182 - loss: 0.6899 \n",
            "Epoch 23/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5414 - loss: 0.6899 \n",
            "Epoch 24/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5215 - loss: 0.6897 \n",
            "Epoch 25/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5182 - loss: 0.6895 \n",
            "Epoch 26/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5331 - loss: 0.6892 \n",
            "Epoch 27/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5215 - loss: 0.6896 \n",
            "Epoch 28/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5364 - loss: 0.6893 \n",
            "Epoch 29/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5364 - loss: 0.6891 \n",
            "Epoch 30/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5199 - loss: 0.6900 \n",
            "Epoch 31/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5199 - loss: 0.6892 \n",
            "Epoch 32/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5199 - loss: 0.6890 \n",
            "Epoch 33/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5248 - loss: 0.6894 \n",
            "Epoch 34/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5281 - loss: 0.6891 \n",
            "Epoch 35/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5331 - loss: 0.6893 \n",
            "Epoch 36/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5513 - loss: 0.6890 \n",
            "Epoch 37/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5397 - loss: 0.6892 \n",
            "Epoch 38/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5331 - loss: 0.6884 \n",
            "Epoch 39/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5315 - loss: 0.6889 \n",
            "Epoch 40/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5248 - loss: 0.6887 \n",
            "Epoch 41/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5397 - loss: 0.6893 \n",
            "Epoch 42/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5480 - loss: 0.6893 \n",
            "Epoch 43/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5199 - loss: 0.6889 \n",
            "Epoch 44/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5364 - loss: 0.6887 \n",
            "Epoch 45/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5397 - loss: 0.6886 \n",
            "Epoch 46/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5430 - loss: 0.6886 \n",
            "Epoch 47/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5315 - loss: 0.6884 \n",
            "Epoch 48/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5315 - loss: 0.6885 \n",
            "Epoch 49/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5464 - loss: 0.6883 \n",
            "Epoch 50/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5215 - loss: 0.6886 \n",
            "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "Accuracy: 0.57\n",
            "Confusion Matrix:\n",
            "[[59 21]\n",
            " [45 27]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Step 1: Load stock data\n",
        "ticker = 'AAPL'\n",
        "df = yf.download(ticker, start='2020-01-01', end='2023-01-01')\n",
        "df['Target'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)\n",
        "\n",
        "# Step 2: Feature selection and scaling\n",
        "features = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(features)\n",
        "y = df['Target'].values\n",
        "\n",
        "# Step 3: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Step 4: Build ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=5, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Step 5: Evaluate model\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 5 ‚Äî Detailed line-by-line explanation (neutral)\n",
        "\n",
        "This markdown explains the contents of the code cell above line-by-line, describes the inputs and outputs at each stage, and summarizes the mathematical concepts, advantages, disadvantages, and applications used in this assignment. No recommendations or changes are included ‚Äî only what the assignment contains and how it is implemented.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Imports (what the code brings in)\n",
        "- `import numpy as np`: imports NumPy for array and numerical operations used throughout the code.\n",
        "- `import pandas as pd`: imports pandas for DataFrame manipulation when handling downloaded stock data.\n",
        "- `import yfinance as yf`: imports yfinance to download historical market data from Yahoo Finance.\n",
        "- `from sklearn.preprocessing import MinMaxScaler`: imports MinMaxScaler to normalize features to a fixed range.\n",
        "- `from sklearn.model_selection import train_test_split`: imports a utility to split data into training and test sets.\n",
        "- `from sklearn.metrics import accuracy_score, confusion_matrix`: imports metrics for classification evaluation.\n",
        "- `from tensorflow.keras.models import Sequential` and `from tensorflow.keras.layers import Dense`: import Keras model and Dense layer to build the ANN architecture used in the assignment.\n",
        "\n",
        "## 2) Step 1 ‚Äî Load stock data and create target\n",
        "- `ticker = 'AAPL'`: sets the ticker symbol for the stock to download (Apple Inc.).\n",
        "- `df = yf.download(ticker, start='2020-01-01', end='2023-01-01')`: downloads daily historical OHLCV (Open, High, Low, Close, Volume) data for the date range and stores it in a pandas DataFrame `df`.\n",
        "- `df['Target'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)`: creates a binary target column where each day's label is 1 if the next day's close is greater than the current day's close, otherwise 0. This uses `shift(-1)` to reference the next row's close price. The resulting `df['Target']` is an array of 0/1 values aligned with rows of `df` (the final row may be undefined due to shift).\n",
        "\n",
        "## 3) Step 2 ‚Äî Feature selection and scaling\n",
        "- `features = df[['Open', 'High', 'Low', 'Close', 'Volume']]`: selects five columns from the DataFrame to be predictor features. The selected features form a table of shape (n_samples, 5).\n",
        "- `scaler = MinMaxScaler()`: creates an instance of the MinMaxScaler. The scaler will map each feature individually to the 0‚Äì1 range using the formula x_scaled = (x - min) / (max - min), where min/max are computed per feature.\n",
        "- `X = scaler.fit_transform(features)`: fits the scaler on the full `features` data and transforms it, returning a NumPy array `X` of scaled feature values (shape (n_samples,5)).\n",
        "- `y = df['Target'].values`: extracts the target values as a NumPy array `y` of shape (n_samples,) with binary values 0 or 1.\n",
        "\n",
        "## 4) Step 3 ‚Äî Train-test split (time-aware)\n",
        "- `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)`: splits the dataset into training and test sets. `test_size=0.2` reserves 20% of samples for testing. `shuffle=False` preserves chronological order so the model is trained on older data and tested on later data (appropriate for time-series forecasting).\n",
        "- Outputs: arrays `X_train`, `y_train` for training and `X_test`, `y_test` for evaluation. Shapes depend on total number of rows in `df` after any implicit filtering.\n",
        "\n",
        "## 5) Step 4 ‚Äî Build ANN model (architecture and forward pass)\n",
        "- `model = Sequential()`: instantiates a Keras Sequential model object that stacks layers in order.\n",
        "- `model.add(Dense(64, input_dim=5, activation='relu'))`: first hidden layer ‚Äî a fully connected layer with 64 neurons, expecting input vectors of length 5. Activation function is ReLU (rectified linear unit). Mathematically, this layer computes z = W^T x + b and outputs a = max(0, z) element-wise.\n",
        "- `model.add(Dense(32, activation='relu'))`: second hidden layer with 32 neurons and ReLU activation; it receives the 64 outputs of the previous layer as its input.\n",
        "- `model.add(Dense(1, activation='sigmoid'))`: output layer with a single neuron and sigmoid activation that maps the final linear output to a probability p ‚àà (0,1), interpreted as the probability that the next day's close will be higher (class 1).\n",
        "\n",
        "## 6) Step 4 (continued) ‚Äî Compile (loss and optimizer) and training\n",
        "- `model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])`: sets the training configuration. The loss function is binary cross-entropy: for a true label y ‚àà {0,1} and predicted probability p the loss per sample is L = ‚àí[y log p + (1 ‚àí y) log(1 ‚àí p)]. The optimizer is Adam, which performs gradient-based updates using adaptive moment estimates. The metric requested is accuracy.\n",
        "- `model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)`: trains the network for 50 epochs over the training set, processing samples in mini-batches of size 32. During training, Keras prints progress including loss and accuracy per epoch (training metrics). The training process updates model weights to minimize the binary cross-entropy loss using backpropagation and the Adam update rule.\n",
        "\n",
        "## 7) Step 5 ‚Äî Model evaluation (prediction and metrics)\n",
        "- `y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")`: generates predicted probabilities on the test set with `model.predict(X_test)` and thresholds them at 0.5 to produce predicted class labels 0 or 1. The boolean result is cast to integers. Output `y_pred` aligns with `y_test` for evaluation.\n",
        "- `accuracy = accuracy_score(y_test, y_pred)`: computes classification accuracy as (number of correct predictions) / (total predictions).\n",
        "- `conf_matrix = confusion_matrix(y_test, y_pred)`: computes the 2√ó2 confusion matrix [[TN, FP], [FN, TP]] indicating counts of true/false positives/negatives.\n",
        "- `print(f\"Accuracy: {accuracy:.2f}\")`, `print(\"Confusion Matrix:\")`, `print(conf_matrix)`: print the numeric accuracy to two decimal places and the confusion matrix array as the final output of the notebook cell.\n",
        "\n",
        "---\n",
        "\n",
        "## Mathematical concepts used in the implementation\n",
        "- Feature scaling (Min‚ÄìMax normalization): x_scaled = (x ‚àí min) / (max ‚àí min). This rescales features to a common range, which aids gradient-based optimization.\n",
        "- Dense (fully connected) layer: computes a linear combination z = W^T x + b followed by a nonlinear activation a = œÜ(z).\n",
        "- ReLU activation: œÜ(z) = max(0, z), a piecewise-linear activation function applied element-wise in hidden layers.\n",
        "- Sigmoid activation (output): œÉ(z) = 1 / (1 + e^{‚àíz}), producing probabilities for binary classification.\n",
        "- Binary cross-entropy loss: L(y,p) = ‚àí[ y log p + (1 ‚àí y) log(1 ‚àí p) ], averaged over samples; used to quantify mismatch between true labels and predicted probabilities.\n",
        "- Backpropagation and gradient descent (Adam optimizer): gradients of loss with respect to weights are computed and used to update weights to reduce loss. Adam uses adaptive estimates of first and second moments of gradients in its update rule.\n",
        "\n",
        "## Inputs and outputs (end-to-end view)\n",
        "- Inputs: historical OHLCV data fetched by yfinance (Open, High, Low, Close, Volume). The features are scaled to [0,1] and split into training and test sets preserving chronological order. Each sample is a 5-dimensional numeric vector.\n",
        "- Intermediate outputs: network activations in hidden layers (64-dim then 32-dim), per-sample predicted probability from the sigmoid output. During training, per-epoch loss and accuracy are printed.\n",
        "- Final outputs: for the test set, predicted binary labels `y_pred`, scalar accuracy, and the 2√ó2 confusion matrix printed to stdout.\n",
        "\n",
        "## Advantages stated in the assignment content\n",
        "- ANNs can model complex, nonlinear relationships present in financial time series, enabling the capture of patterns that simpler linear models might miss.\n",
        "- With appropriate features and training, ANNs can provide probabilistic outputs (via the sigmoid) useful for decision-making or downstream analysis.\n",
        "- The assignment demonstrates how a feedforward ANN can be applied to a next-day movement (up/down) classification task using tabular financial features.\n",
        "\n",
        "## Disadvantages / limitations stated in the assignment content\n",
        "- ANNs require sufficient and relevant historical data to train effectively. Small datasets or poor features limit performance.\n",
        "- Training neural networks can be computationally expensive, particularly for larger architectures or long training runs.\n",
        "- Risk of overfitting: the model may learn idiosyncrasies of the training data and fail to generalize to new market conditions. Financial data is non-stationary and noisy, which challenges predictive performance.\n",
        "\n",
        "## Applications mentioned in the assignment content\n",
        "- Short-term stock movement prediction (next-day up/down classification).\n",
        "- Trading decision support where predicted probabilities or labels inform buy/sell decisions.\n",
        "- Financial forecasting tasks where a binary outcome is of interest and ANN models can be used as one component among broader analytic systems.\n",
        "\n",
        "---\n",
        "\n",
        "End of neutral, detailed explanation for Assignment 5: the content above reflects the code cell implementation, the inputs/outputs at each step, the key mathematical concepts used, and the assignment-level advantages, disadvantages, and applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Excellent ‚Äî this code builds an **Artificial Neural Network (ANN)** to predict **next-day stock movement** (up or down) for Apple (AAPL) using **historical price data**. Let‚Äôs carefully unpack every step, from data acquisition to evaluation, including reasoning, assumptions, and improvement opportunities.\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Importing Required Libraries\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "```\n",
        "\n",
        "**What each library does:**\n",
        "\n",
        "| Library                   | Purpose                                                   |\n",
        "| ------------------------- | --------------------------------------------------------- |\n",
        "| `numpy`                   | Numerical operations and array handling                   |\n",
        "| `pandas`                  | Data manipulation, especially with tabular data           |\n",
        "| `yfinance`                | Downloads real stock market data from Yahoo Finance       |\n",
        "| `sklearn.preprocessing`   | Scaling features to normalize magnitudes                  |\n",
        "| `sklearn.model_selection` | Splitting data into training/testing sets                 |\n",
        "| `sklearn.metrics`         | Evaluating model performance (accuracy, confusion matrix) |\n",
        "| `tensorflow.keras`        | Building and training the ANN model                       |\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ Step 1: Load Stock Data\n",
        "\n",
        "```python\n",
        "ticker = 'AAPL'\n",
        "df = yf.download(ticker, start='2020-01-01', end='2023-01-01')\n",
        "df['Target'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* `yf.download()` fetches Apple Inc. (AAPL) stock prices from Jan 2020 to Jan 2023.\n",
        "* Data includes columns: `Open`, `High`, `Low`, `Close`, `Adj Close`, `Volume`.\n",
        "* A new column **Target** is created:\n",
        "\n",
        "  * **1** ‚Üí if next day‚Äôs closing price is higher (price goes up)\n",
        "  * **0** ‚Üí if next day‚Äôs closing price is lower (price goes down)\n",
        "\n",
        "So this is a **binary classification problem**:\n",
        "‚Üí Predict whether tomorrow‚Äôs price will rise or fall.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "| Close (today) | Close (tomorrow) | Target |\n",
        "| ------------- | ---------------- | ------ |\n",
        "| 150.00        | 151.20           | 1      |\n",
        "| 151.20        | 149.80           | 0      |\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Step 2: Feature Selection and Scaling\n",
        "\n",
        "```python\n",
        "features = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(features)\n",
        "y = df['Target'].values\n",
        "```\n",
        "\n",
        "**Why these features?**\n",
        "\n",
        "* These 5 are standard OHLCV (Open, High, Low, Close, Volume) ‚Äî basic technical indicators of stock behavior.\n",
        "\n",
        "**Scaling with MinMaxScaler:**\n",
        "\n",
        "* Neural networks work best when inputs are normalized between [0, 1].\n",
        "* `fit_transform()` learns the min‚Äìmax range of each feature and scales them accordingly.\n",
        "\n",
        "**Result:**\n",
        "\n",
        "* `X`: 2D NumPy array (scaled features)\n",
        "* `y`: 1D array (0 or 1 target labels)\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ Step 3: Train-Test Split\n",
        "\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "```\n",
        "\n",
        "**Key points:**\n",
        "\n",
        "* Splits dataset into:\n",
        "\n",
        "  * **80% training data**\n",
        "  * **20% test data**\n",
        "* `shuffle=False` preserves *time order* ‚Äî crucial for time-series prediction, to avoid ‚Äúpeeking into the future.‚Äù\n",
        "\n",
        "**Note:**\n",
        "Training data = earlier dates\n",
        "Testing data = later dates (more realistic scenario)\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Step 4: Build the Artificial Neural Network\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=5, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "```\n",
        "\n",
        "### Architecture breakdown:\n",
        "\n",
        "| Layer          | Neurons             | Activation | Purpose                                    |\n",
        "| -------------- | ------------------- | ---------- | ------------------------------------------ |\n",
        "| Input Layer    | 5 (from 5 features) | ‚Äì          | Receives OHLCV data                        |\n",
        "| Hidden Layer 1 | 64                  | ReLU       | Learns non-linear relationships            |\n",
        "| Hidden Layer 2 | 32                  | ReLU       | Refines deeper feature interactions        |\n",
        "| Output Layer   | 1                   | Sigmoid    | Outputs probability (0‚Äì1) for binary class |\n",
        "\n",
        "### Why ReLU + Sigmoid?\n",
        "\n",
        "* **ReLU (Rectified Linear Unit):** avoids vanishing gradient, efficient for hidden layers.\n",
        "* **Sigmoid:** converts output into probability ‚Äî perfect for binary classification.\n",
        "\n",
        "---\n",
        "\n",
        "### Compile the Model\n",
        "\n",
        "```python\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* **Loss = binary_crossentropy**: standard for 2-class classification.\n",
        "* **Optimizer = Adam**: adaptive learning rate optimizer, fast and stable.\n",
        "* **Metrics = accuracy**: evaluate how often the model predicts correctly.\n",
        "\n",
        "---\n",
        "\n",
        "### Train the Model\n",
        "\n",
        "```python\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "```\n",
        "\n",
        "**Training parameters:**\n",
        "\n",
        "* **epochs = 50:** one epoch = one full pass over training data.\n",
        "* **batch_size = 32:** updates weights after every 32 samples.\n",
        "* **verbose = 1:** shows progress output.\n",
        "\n",
        "The model iteratively adjusts weights to minimize loss, improving accuracy on the training data.\n",
        "\n",
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ Step 5: Evaluate the Model\n",
        "\n",
        "```python\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* `model.predict()` outputs probabilities between 0 and 1.\n",
        "* `> 0.5` converts probabilities to binary class predictions (1 or 0).\n",
        "* **Accuracy:** proportion of correct predictions.\n",
        "* **Confusion Matrix:** gives detailed breakdown:\n",
        "\n",
        "|              | Predicted ‚Üì / Actual ‚Üí | 0               | 1 |\n",
        "| ------------ | ---------------------- | --------------- | - |\n",
        "| **0 (down)** | True Negatives         | False Positives |   |\n",
        "| **1 (up)**   | False Negatives        | True Positives  |   |\n",
        "\n",
        "---\n",
        "\n",
        "### Print Results\n",
        "\n",
        "```python\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "```\n",
        "\n",
        "Example output:\n",
        "\n",
        "```\n",
        "Accuracy: 0.58\n",
        "Confusion Matrix:\n",
        "[[130  70]\n",
        " [ 90 110]]\n",
        "```\n",
        "\n",
        "This means the model correctly predicted market direction 58% of the time ‚Äî slightly above random chance, which is typical for raw OHLC-based models.\n",
        "\n",
        "---\n",
        "\n",
        "## 7Ô∏è‚É£ Conceptual Summary\n",
        "\n",
        "| Step                    | Description                                     | Tools Used                           |\n",
        "| ----------------------- | ----------------------------------------------- | ------------------------------------ |\n",
        "| **Data Fetching**       | Get historical prices                           | `yfinance`                           |\n",
        "| **Feature Engineering** | Create input features and binary target         | `numpy`, `pandas`                    |\n",
        "| **Scaling**             | Normalize features to [0,1]                     | `MinMaxScaler`                       |\n",
        "| **Train/Test Split**    | Separate old vs new data                        | `train_test_split`                   |\n",
        "| **ANN Model**           | Learn non-linear mapping from prices ‚Üí movement | `Sequential`, `Dense`                |\n",
        "| **Evaluation**          | Measure accuracy & confusion                    | `accuracy_score`, `confusion_matrix` |\n",
        "\n",
        "---\n",
        "\n",
        "## 8Ô∏è‚É£ Key Assumptions\n",
        "\n",
        "1. **Next-day movement** depends only on today‚Äôs OHLCV ‚Äî ignores trend memory or external signals.\n",
        "2. **Stock market** is *not fully random* ‚Äî assuming patterns exist that can be learned.\n",
        "3. **Stationarity** ‚Äî assumes statistical properties don‚Äôt drastically change (not always true in markets).\n",
        "4. **No lookahead bias** ‚Äî ensured by `shuffle=False` (good practice).\n",
        "\n",
        "---\n",
        "\n",
        "## 9Ô∏è‚É£ Limitations & Potential Improvements\n",
        "\n",
        "| Limitation                | Improvement                                                          |\n",
        "| ------------------------- | -------------------------------------------------------------------- |\n",
        "| Uses only raw OHLCV       | Add **technical indicators** (e.g., RSI, MACD, EMA, Bollinger Bands) |\n",
        "| Ignores temporal sequence | Use **LSTM/GRU** (recurrent neural networks) for time-dependence     |\n",
        "| No regularization         | Add **Dropout** layers to prevent overfitting                        |\n",
        "| No model validation       | Use **validation_split** in training                                 |\n",
        "| Binary target only        | Try **regression** for predicting actual % change                    |\n",
        "| Fixed threshold (0.5)     | Use **ROC curve** or tune threshold for precision-recall tradeoff    |\n",
        "\n",
        "---\n",
        "\n",
        "## üîü Intuitive Summary\n",
        "\n",
        "This script builds a **feedforward neural network** that tries to ‚Äúguess‚Äù whether Apple‚Äôs price will go *up or down* tomorrow based solely on today‚Äôs market stats.\n",
        "\n",
        "While it‚Äôs a good **introductory machine learning experiment**, it‚Äôs far from a production-grade trading model ‚Äî but forms the foundation for deeper time-series or reinforcement learning approaches.\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to show a **next-level version** of this ‚Äî one that includes **technical indicators**, **LSTM-based memory**, and **improved evaluation (precision, recall, ROC, etc.)** ‚Äî while keeping the logic readable?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
